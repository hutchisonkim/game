name: Spark Tests (Prebuilt Image)

on:
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest

    permissions:
      contents: read

    container:
      image: ghcr.io/${{ github.repository_owner }}/spark-dotnet:3.5.3-2.3.0
      options: --init
      credentials:
        username: ${{ github.repository_owner }}
        password: ${{ secrets.GHCR_PAT }}

    env:
      SPARK_HOME: /opt/spark
      SPARK_VERSION: "3.5.3"
      DOTNET_WORKER_DIR: /opt/microsoft-spark-worker
      DOTNET_CLI_TELEMETRY_OPTOUT: '1'
      DOTNET_CLI_HOME: /github/home
      NUGET_PACKAGES: /github/home/.nuget
      DOTNETBACKEND_PORT: "5567"

    steps:
      - uses: actions/checkout@v4

      - name: Restore Test Project Packages (Spark DLL fix - 1 of 3)
        run: |
          echo "=== Restoring Project Packages ==="
          set -e
          cd tests/Game.Chess.Tests.Integration
          dotnet restore Game.Chess.Tests.Integration.csproj --runtime linux-x64

      - name: Fix Windows-style paths in NuGet cache (Spark DLL fix - 2 of 3)
        shell: bash
        run: |
          echo "=== Checking for Windows-style paths in NuGet packages ==="

          # Depth-first, handle files/dirs with backslashes
          find "$NUGET_PACKAGES" -depth -name '*\\*' -print0 | while IFS= read -r -d '' f; do
            # Convert backslashes to forward slashes
            NEW_PATH=$(echo "$f" | sed 's/\\/\//g')
            # Strip trailing slashes so Linux handles files correctly
            NEW_PATH="${NEW_PATH%/}"

            if [ -d "$f" ]; then
              mkdir -p "$NEW_PATH"
              mv "$f"/* "$NEW_PATH"/ 2>/dev/null || true
              rmdir "$f" 2>/dev/null || true
              echo "Fixed directory: $f -> $NEW_PATH"
            else
              mkdir -p "$(dirname "$NEW_PATH")"
              mv "$f" "$NEW_PATH"
              echo "Fixed file: $f -> $NEW_PATH"
            fi
          done

          # Verify Microsoft.Spark.dll exists after fixing paths
          SPARK_DLL="$NUGET_PACKAGES/microsoft.spark/2.3.0/lib/netstandard2.1/Microsoft.Spark.dll"
          if [ -f "$SPARK_DLL" ]; then
            echo "✅ Microsoft.Spark.dll confirmed at $SPARK_DLL"
          else
            echo "❌ Still missing Microsoft.Spark.dll at $SPARK_DLL"
            exit 1
          fi

      - name: Build Test Project (Spark DLL fix - 3 of 3)
        run: |
          set -e
          cd tests/Game.Chess.Tests.Integration
          dotnet build Game.Chess.Tests.Integration.csproj -c Release -v d

      - name: Build Spark Runner
        run: |
          set -e
          cd tests/Game.Chess.Tests.Integration.Runner
          dotnet build Game.Chess.Tests.Integration.Runner.csproj -c Release --runtime linux-x64

      - name: Start Spark Runner
        shell: pwsh
        env:
          DOTNET_WORKER_DIR: /opt/microsoft-spark-worker
          DOTNETBACKEND_PORT: "5567"
          DOTNET_WORKER_SPARK_VERSION: "2.3.0"
          PYTHON_WORKER_FACTORY_PORT: "5567"
        run: pwsh ./scripts/spark-runner-start.ps1

      - name: Run Tests
        shell: pwsh
        env:
          DOTNET_WORKER_DIR: /opt/microsoft-spark-worker
          DOTNETBACKEND_PORT: "5567"
          DOTNET_WORKER_SPARK_VERSION: "2.3.0"
          PYTHON_WORKER_FACTORY_PORT: "5567"
        run: pwsh ./scripts/run-tests-with-dependencies.ps1 -Filter "Essential=true"

      - name: Find TRX file (diagnostic)
        if: always()
        shell: bash
        run: |
          echo "Searching for TRX files..."
          find . -name "*.trx" -type f 2>/dev/null || echo "No TRX files found"
          echo "Searching for TestResults directories..."
          find . -name "TestResults" -type d 2>/dev/null || echo "No TestResults directories found"

      - name: Upload xUnit TRX logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: xunit-trx-logs
          path: "**/*.trx"
          
      - name: Summarize TRX Results
        if: always()
        shell: bash
        run: |
          # Find all TRX files
          TRX_FILES=$(find . -name "*.trx" -type f 2>/dev/null)
          
          if [ -z "$TRX_FILES" ]; then
            echo "⚠️ No test results found"
            exit 0
          fi
          
          echo "Found TRX files:"
          echo "$TRX_FILES"
          
          # Initialize counters
          TOTAL_PASS=0
          TOTAL_FAIL=0
          FAILURES=""
          
          for TRX in $TRX_FILES; do
            # Strip BOM (xmllint pre-installed in Docker image)
            perl -pe 's/\xEF\xBB\xBF// if $. == 1' "$TRX" > /tmp/clean.trx
            
            # Parse results
            PASS=$(xmllint --xpath "count(//*[local-name()='UnitTestResult' and @outcome='Passed'])" /tmp/clean.trx 2>/dev/null || echo 0)
            FAIL=$(xmllint --xpath "count(//*[local-name()='UnitTestResult' and @outcome='Failed'])" /tmp/clean.trx 2>/dev/null || echo 0)
            
            TOTAL_PASS=$((TOTAL_PASS + PASS))
            TOTAL_FAIL=$((TOTAL_FAIL + FAIL))
            
            if [ "$FAIL" -gt 0 ]; then
              TEST_NAME=$(xmllint --xpath "//*[local-name()='UnitTestResult' and @outcome='Failed']/@testName" /tmp/clean.trx 2>/dev/null | grep -o 'testName="[^"]*"' | cut -d'"' -f2 | head -1)
              FAILURES="${FAILURES}* $TEST_NAME\n"
            fi
          done
          
          # Write summary
          {
            echo "## Test Results"
            echo "**Passed:** $TOTAL_PASS | **Failed:** $TOTAL_FAIL"
            
            if [ "$TOTAL_FAIL" -gt 0 ]; then
              echo ""
              echo "### Failures"
              echo -e "$FAILURES"
            fi
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Cleanup Spark Runner
        if: always()
        shell: pwsh
        run: |
          $stopJob = Start-Job -ScriptBlock { pwsh scripts/spark-testctl.ps1 -Stop }
          if (-not (Wait-Job -Job $stopJob -Timeout 10)) {
            Stop-Job -Job $stopJob
          } else {
            $stopJob | Receive-Job | Out-Null
          }
          Start-Sleep -Seconds 2
          pwsh scripts/force-kill-runners.ps1 | Out-Null
          Get-Process java -ErrorAction SilentlyContinue | Stop-Process -Force -ErrorAction SilentlyContinue
          Write-Host '✓ Cleanup complete'


