name: Spark Tests (Prebuilt Image)

on:
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  test:
    runs-on: ubuntu-latest

    permissions:
      contents: read

    container:
      image: ghcr.io/${{ github.repository_owner }}/spark-dotnet:3.5.3-2.3.0
      options: --init
      credentials:
        username: ${{ github.repository_owner }}
        password: ${{ secrets.GHCR_PAT }}

    env:
      SPARK_HOME: /opt/spark
      SPARK_VERSION: "3.5.3"
      DOTNET_WORKER_DIR: /opt/microsoft-spark-worker
      DOTNET_CLI_TELEMETRY_OPTOUT: '1'
      DOTNET_CLI_HOME: /github/home
      NUGET_PACKAGES: /github/home/.nuget
      DOTNETBACKEND_PORT: "5567"

    steps:
      - uses: actions/checkout@v4

      - name: Clear and prepare NuGet caches
        run: |
          dotnet nuget locals all --clear
          mkdir -p "$NUGET_PACKAGES"
          chmod -R 777 "$NUGET_PACKAGES"

      - name: Restore Spark tests
        run: |
          set -e
          cd tests/Game.Chess.Tests.Integration.Runner
          dotnet restore Game.Chess.Tests.Integration.Runner.csproj --runtime linux-x64

      - name: Fix Windows-style paths in NuGet cache
        shell: bash
        run: |
          echo "=== Checking for Windows-style paths in NuGet packages ==="

          # Depth-first, handle files/dirs with backslashes
          find "$NUGET_PACKAGES" -depth -name '*\\*' -print0 | while IFS= read -r -d '' f; do
            # Convert backslashes to forward slashes
            NEW_PATH=$(echo "$f" | sed 's/\\/\//g')
            # Strip trailing slashes so Linux handles files correctly
            NEW_PATH="${NEW_PATH%/}"

            if [ -d "$f" ]; then
              mkdir -p "$NEW_PATH"
              mv "$f"/* "$NEW_PATH"/ 2>/dev/null || true
              rmdir "$f" 2>/dev/null || true
              echo "Fixed directory: $f -> $NEW_PATH"
            else
              mkdir -p "$(dirname "$NEW_PATH")"
              mv "$f" "$NEW_PATH"
              echo "Fixed file: $f -> $NEW_PATH"
            fi
          done

          # Verify Microsoft.Spark.dll exists after fixing paths
          SPARK_DLL="$NUGET_PACKAGES/microsoft.spark/2.3.0/lib/netstandard2.1/Microsoft.Spark.dll"
          if [ -f "$SPARK_DLL" ]; then
            echo "✅ Microsoft.Spark.dll confirmed at $SPARK_DLL"
          else
            echo "❌ Still missing Microsoft.Spark.dll at $SPARK_DLL"
            exit 1
          fi

      - name: Install PowerShell
        run: |
          apt-get update -qq && apt-get install -y -qq wget apt-transport-https software-properties-common
          wget -q "https://packages.microsoft.com/config/ubuntu/$(lsb_release -rs)/packages-microsoft-prod.deb"
          dpkg -i packages-microsoft-prod.deb 2>&1 | grep -v "^Selecting\|^(Reading\|^Unpacking\|^Setting" || true
          apt-get update -qq
          apt-get install -y -qq powershell

      - name: Install system dependencies
        run: |
          apt-get install -y -qq netcat-openbsd zip net-tools

      - name: Start Spark Runner
        shell: pwsh
        env:
          DOTNET_WORKER_DIR: /opt/microsoft-spark-worker
          DOTNETBACKEND_PORT: "5567"
          DOTNET_WORKER_SPARK_VERSION: "2.3.0"
          PYTHON_WORKER_FACTORY_PORT: "5567"
        run: pwsh ./scripts/spark-runner-start.ps1

      - name: Run Tests
        shell: pwsh
        env:
          DOTNET_WORKER_DIR: /opt/microsoft-spark-worker
          DOTNETBACKEND_PORT: "5567"
          DOTNET_WORKER_SPARK_VERSION: "2.3.0"
          PYTHON_WORKER_FACTORY_PORT: "5567"
        run: pwsh ./scripts/run-tests-with-dependencies.ps1 -Filter "Essential=true"

      - name: Upload xUnit TRX logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: xunit-trx-logs
          path: tests/Game.Chess.Tests.Integration.Runner/bin/Release/net8.0/linux-x64/publish/TestResults/TestResults.trx
          
      - name: Summarize TRX Results
        if: always()
        shell: bash
        run: |
          TRX="tests/Game.Chess.Tests.Integration.Runner/bin/Release/net8.0/linux-x64/publish/TestResults/TestResults.trx"
          
          if [ ! -f "$TRX" ]; then
            echo "⚠️ No test results found"
            exit 0
          fi

          # Strip BOM and install xmllint if needed
          perl -pe 's/\xEF\xBB\xBF// if $. == 1' "$TRX" > /tmp/clean.trx
          command -v xmllint >/dev/null || apt-get install -y -qq libxml2-utils

          # Parse results
          PASS=$(xmllint --xpath "count(//*[local-name()='UnitTestResult' and @outcome='Passed'])" /tmp/clean.trx 2>/dev/null || echo 0)
          FAIL=$(xmllint --xpath "count(//*[local-name()='UnitTestResult' and @outcome='Failed'])" /tmp/clean.trx 2>/dev/null || echo 0)

          # Write summary
          {
            echo "## Test Results"
            echo "**Passed:** $PASS | **Failed:** $FAIL"
            
            if [ "$FAIL" -gt 0 ]; then
              echo ""
              echo "### Failures"
              xmllint --xpath "//*[local-name()='UnitTestResult' and @outcome='Failed']/@testName" /tmp/clean.trx 2>/dev/null \
                | grep -o 'testName="[^"]*"' | cut -d'"' -f2 | sed 's/^/* /' || true
            fi
          } >> "$GITHUB_STEP_SUMMARY"

      - name: Cleanup Spark Runner
        if: always()
        shell: pwsh
        run: |
          $stopJob = Start-Job -ScriptBlock { pwsh scripts/spark-testctl.ps1 -Stop }
          if (-not (Wait-Job -Job $stopJob -Timeout 10)) {
            Stop-Job -Job $stopJob
          } else {
            $stopJob | Receive-Job | Out-Null
          }
          Start-Sleep -Seconds 2
          pwsh scripts/force-kill-runners.ps1 | Out-Null
          Get-Process java -ErrorAction SilentlyContinue | Stop-Process -Force -ErrorAction SilentlyContinue
          Write-Host '✓ Cleanup complete'


